# TODO physical scale of the data
# TODO discuss the depenendencies more closely
# TODO discuss the training, description of the training process, training data

name: UNet2dExample
description: A 2d U-Net pretrained on broad nucleus dataset.
# we allow for multiple citations. Each citation contains TEXT, DOI and URL. One of DOI or URL needs to be given.
cite:
    - text: "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015."
      doi: https://doi.org/10.1007/978-3-319-24574-4_28
license: MIT
authors:
    - Constantin Pape;@constantinpape
    - Fynn Beutenmueller
documentation: ./unet2d.md
tags: [unet2d, pytorch, nucleus-segmentation]
covers: []

format_version: 0.2.0
language: python
framework: pytorch

model:
    source: ./unet2d.py:UNet2d
    kwargs: {input_channels: 1, output_channels: 1}

weights:
    - id: default
      name: Default Weights
      # TODO how do we make clear that this is a doi? doi:.... ?
      source: 10.5281/zenodo.3446812
      sha256: e4d3885bccbe41cbf6c1d825f3cd2b707c7021ead5593156007e407a16b27cf2
      timestamp: 2019-12-11T12:22:32.11
      authors: ["author1", "author2"]
      covers: []
      description: "Weights from DataScienceBowl"
      # These should be different they are not
    - id: my_other_weights
      name: My Other Weights
      source: 10.5281/zenodo.3446812
      sha256: e4d3885bccbe41cbf6c1d825f3cd2b707c7021ead5593156007e407a16b27cf2
      parent: default
      timestamp: 2019-12-14T21:59:43.10
      authors: ["author2"]
      covers: []
      description: "Fine tuned weights based on DataScienceBowl"

test_input: my_test_input.npy  # language + extension defines memory representation
test_output: my_test_output.npy

# optional:
inputs: # needs to become ordered dict (same for output) or list of dicts should contain names to
    # discussion about NCHWD versus bxyzt this also has implications on the location of the origin and the
  - name: input
    axes: bcyx #btczyx
    data_type: float32
    data_range: [-inf, inf]
    shape: [1, 1, 512, 512]

outputs:
  - name: logits
    description: network probabilities  # optional description
    axes: bcyx
    data_type: float32
    data_range: [-inf, inf]
    halo: [0, 0, 32, 32]
    shape:
        reference_input: input
        scale: [1, 1, 1, 1]
        offset: [0, 0, 0, 0]

prediction:
    preprocess:
        - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/EnsureTorch.transformation.yaml
        - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/Cast.transformation.yaml
          kwargs: {dtype: float32}
        - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/NormalizeZeroMeanUnitVariance.transformation.yaml
          kwargs: {apply_to: [0]}
    weights: default
    postprocess:
        - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/Sigmoid.transformation.yaml
        - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/EnsureNumpy.transformation.yaml
    dependencies: conda:./environment.yaml

training:
  setup:
#    meta_sampler:
#      spec:
#      kwargs:
    samplers:
      - spec: https://github.com/bioimage-io/python-bioimage-io/blob/b72f9743f25367ee5d70d699551bd03013765b1f/specs/samplers/SequentialSamplerAlongDimension.sampler.yaml
        kwargs: {sample_dimensions: [0, 0]}
        readers:
          - spec: https://github.com/bioimage-io/python-bioimage-io/blob/b72f9743f25367ee5d70d699551bd03013765b1f/specs/readers/BroadNucleusDataBinarized.reader.yaml
            transformations:
              - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/NormalizeZeroMeanUnitVariance.transformation.yaml
                kwargs: {apply_to: [0]}
    preprocess: []
    postprocess:
      - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/Sigmoid.transformation.yaml
        kwargs: {apply_to: [0]}
      - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/Cast.transformation.yaml
        kwargs: {apply_to: [1], dtype: float32}
    losses:
      - spec: https://github.com/bioimage-io/pytorch-bioimage-io/blob/8959dab1236a72593613a5c89ea973c2f5612aea/specs/transformations/BCELoss.transformation.yaml
    optimizer:
      source: torch.optim.Adam
      required_kwargs: [params]
      optional_kwargs: {lr: 0.002}
    # validation:
    #   - {}

  source: pybio.torch.training.simple.simple_training
  required_kwargs: [pybio_model]
  optional_kwargs: {n_iterations: 25, batch_size: 4, num_workers: 2, out_file: ./weights.pytorch}
  # enable different ways of specifying the dependencies.
  # this would hold all training dependencies, e.g. as a frozen conda environment
  # or as a pom.xml
  dependencies: # this is a file to the dependencies
      conda:../environment.yaml
  description: Train the unet via binary cross entropy
