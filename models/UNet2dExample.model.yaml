# TODO physical scale of the data
# TODO discuss the depenendencies more closely
# TODO discuss the training, description of the training process, training data

name: UNet2dExample
description: A 2d U-Net pretrained on broad nucleus dataset.
# we allow for multiple citations. Each citation contains TEXT, DOI and URL. One of DOI or URL needs to be given.
cite:
    - text: "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015."
      doi: https://doi.org/10.1007/978-3-319-24574-4_28
authors:
    - Constantin Pape;@constantinpape
    - Fynn Beutenmueller
documentation: ./unet2d.md
tags: [unet2d, pytorch, nucleus-segmentation]

format_version: 0.1.0
language: python
framework: pytorch

source: ./unet2d.py:UNet2d
kwargs: {input_channels: 1, output_channels: 1}

test_input: ./my_test_input.npy  # language + extension defines memory representation
test_output: ./my_test_output.npy

# optional:
thumbnail: ./my_thumbnail.png

inputs: # needs to become ordered dict (same for output) or list of dicts should contain names to
    # discussion about NCHWD versus bxyzt this also has implications on the location of the origin and the
  - name: input1
    axes: bcyx #btczyx
    data_type: float32
    data_range: [-inf, inf]
    shape:
        min: [1, 1, 32, 32]
        step: [null, 0, 32, 32]
        # exact: [null, 1, 256, 256]  # exact can also be list
outputs:
  - name: output1
    description: network probabilities  # optional description
    axes: bcyx
    data_type: float32
    data_range: [0, 1]
    shape:
        reference_input: input1
        # output_shape = input_shape * scale + offset
        scale: [1, 1, 1, 1]
        offset: [0, 0, 0, 0]
        # exact: [...]  # exact can also be list
        halo: [0, 0, 32, 32] # halo tells you what to crop from the output image

# discussion: do we support a single or multiple weights?
# example kipoi: ginger templating
# list for ensembles?
# FOR NOW: One model one weight file
prediction:
    preprocess:
        - spec: ../transformations/NormalizeZeroMeanUnitVariance.transformation.yaml
          kwargs:
              eps: 1.0e-6  # currently all kwargs need to be specified (here or in the consumer)
              apply_to: [0]
          # version: 0.1.0  # required for transforms from core
    weights:
        # TODO how do we make clear that this is a doi? doi:.... ?
        source: 10.5281/zenodo.3446812
        hash: {md5: c16cb3ba3cb9d257550fd19067ecfb91}
    postprocess: null
    dependencies: conda:./environment.yaml  # this is a file to the dependencies

#
training:
    setup:
        reader:
            spec: ../readers/nN5.reader.yaml
            kwargs: {uri: https://www.mydata.com/my_test_file.n5}
        sampler:
            spec: ../samplers/RandomBatch.sampler.yaml
            kwargs: {seed: 123, output_shape: [1, 2, 256, 256], overlap: [0, 0, 100, 100]}
        preprocess:
            - spec: ../transformations/NormalizeZeroMeanUnitVariance.transformation.yaml
              kwargs: {eps: 1.0e-6, apply_to: [0]}
        loss:
            - spec: ../transformations/Sigmoid.transformation.yaml
              kwargs: {apply_to: [0]}
            - spec: ../transformations/BCELoss.transformation.yaml
        optimizer:
            source: torch.optim.Adam  # TODO do we need a yaml for this too ?
            kwargs: {lr: 0.002}
        # validation:
        #   - {}
        # callback_engine:
        #    - {}
    source: ./train.py:my_train_function
    kwargs: {n_epochs: 100}
    # enable different ways of specifying the dependencies.
    # this would hold all training dependencies, e.g. as a frozen conda environment
    # or as a pom.xml
    dependencies: # this is a file to the dependencies
        conda:./environment.yaml
    description: "Train the unet via binary cross entropy"
